{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本日作業請完整閱讀以下任一文獻即可：\n",
    "\n",
    "* Kaggle 大師帶你了解梯度提升機原理 - 英文 http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "* 完整的 Ensemble 概念 by 李宏毅教授 https://www.youtube.com/watch?v=tH9FH1DH5n0\n",
    "* 深入了解 Gradient-boosting - 英文 https://explained.ai/gradient-boosting/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from Kaggle 大師帶你了解梯度提升機原理\n",
    "約略理解 Gradient Boosting 概念，用預測的偏差值，進行組合應用\n",
    "\n",
    "# 完整的 Ensemble 概念 by 李宏毅教授\n",
    "李教授的說明很清楚，我能理解。Ensemble=打群架的比喻很棒！\n",
    "Model複雜容易overfit則採用bagging (e.g. decision tree, 很容易達到100%正確率, 就是overfitting)。Bagging樹層級越多越好。\n",
    "Random forrest可以不用切validation set, 它本身就有out-of-bag validation。\n",
    "Boosting用在弱的model，樹越多越好。保障錯誤率可以達到0%。序列地找classifier functions, 第二個跟第一個互補，第三個跟第二個互補... 以此類推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
